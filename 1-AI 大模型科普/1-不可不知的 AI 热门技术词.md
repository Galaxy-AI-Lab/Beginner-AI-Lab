# 不可不知的 AI 热门技术词

在 AI 快速发展的今天，我们经常会看到一些专业术语让人摸不着头脑。下面让我们一起来认识一些常见的 AI 技术词汇吧！

## 大语言模型 (LLM，Large Language Model)

大语言模型就像是一个超级厉害的"语言专家"。它通过学习海量的文字资料，可以理解和生成人类语言。举个例子：

- 就像 DeepSeek 可以和你聊天、写文章、回答问题
- 就像有一个无所不知的朋友，你问什么它都能回答

常见的大语言模型有：

- OpenAI 的 GPT-4、GPT-3.5
- 国产的文心一言、讯飞星火
- 开源的 Llama2、ChatGLM

## Prompt（提示词）

Prompt 就是你对 AI 说的"指令"。就像你在餐厅向服务员点餐：

- 你说"我要一杯热咖啡，不加糖"
- 对 AI 说"帮我写一篇关于春天的诗"

提示词的艺术：

- 要清晰具体：说明你想要什么、不想要什么
- 可以分步骤：把复杂任务拆分成简单步骤
- 要有上下文：提供必要的背景信息
- 可以给示例：通过例子说明你想要的格式或风格

## Token

Token 是 AI 理解文字的最小单位。比如：

- "我爱你"这句话可能被分成"我"、"爱"、"你"三个 token
- 英文 "I love you" 可能被分成 "I"、"love"、"you" 三个 token

Token 的作用如下：

- Token 可以用来计算文本长度
- Token 可以帮助 AI 理解文本的语义
- Token 是构建语言模型的基础

不同的模型对 token 的处理方式不同，我们可以在 [Tiktokenizer](https://tiktokenizer.vercel.app/) 网站上查看文本的 token 分割情况。

## Fine-tuning（微调）

Fine-tuning 就像是给 AI 开"补习班"。比如：

- 原本的 AI 只会普通对话
- 经过医疗知识的微调训练后，就能专门回答医疗问题

微调的特点：

- 可以让 AI 学习特定领域的专业知识
- 能够遵循特定的回答格式和风格
- 相比原始模型，响应更快更准确
- 需要准备高质量的训练数据

## Embedding（嵌入）

Embedding 就是把文字转换成 AI 能理解的数字（向量）。想象：

- 把"猫"和"狗"转成数字后，它们的数字会很接近，因为都是宠物
- 把"猫"和"电脑"转成数字后，数字会相距很远，因为概念差异大

实际应用：

- 可以用来做相似文本搜索
- 帮助 AI 理解文本的语义关系
- 是构建智能问答系统的基础
- 常用于文档检索和推荐系统

## RAG (检索增强生成，Retrieval-Augmented Generation)

RAG 就像是给 AI 配了一个"智能笔记本"：

- AI 回答问题时会先查阅笔记本里的资料
- 然后结合查到的资料来回答
- 这样可以让 AI 的回答更准确，不会胡说八道

RAG 的优势：

- 可以利用最新资料，不受训练数据时间限制
- 回答更有依据，可以提供信息来源
- 适合构建企业知识库问答系统
- 能有效降低 AI 的幻觉问题

## 向量数据库

向量数据库就像是一个超级图书馆：

- 可以存储并快速检索 AI 理解的数字化信息
- 比如你问"关于猫的问题"，它能立刻找出所有和猫相关的资料
- 帮助 AI 快速找到需要的信息

常见的向量数据库：

- [Milvus](https://milvus.io/)：开源的分布式向量数据库
- [Pinecone](https://www.pinecone.io/)：专门为 AI 应用设计的向量数据库
- [Chroma](https://www.trychroma.com/)：轻量级的向量数据库，适合个人开发
- [FAISS](https://faiss.ai/)：Facebook 开源的向量检索库

## Attention（注意力机制）

Attention 机制就像人类阅读时的关注重点：

- 读"我很喜欢吃苹果"时
- AI 会特别注意"喜欢"和"苹果"的关系
- 帮助 AI 更好地理解句子的含义

工作原理：

- 计算每个词与其他词的关联程度
- 找出上下文中最重要的信息
- 它是 Transformer 架构的重要组成部分
